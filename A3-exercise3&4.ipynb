{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Implement minibatch SGD to train a binary Logistic Regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement that, we write a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class LogisticRegressionSGD:\n",
    "    def __init__(self, learning_rate=0.01, batch_size=32, max_iters=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iters = max_iters\n",
    "        self.weights = None\n",
    "        # scales the data to 0 mean and unit variance to avoid overflow\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def compute_loss(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        y_pred = self.sigmoid(X.dot(self.weights))\n",
    "        #add epsilon to avoid log 0\n",
    "        epsilon = 1e-10\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        loss = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "        return loss\n",
    "\n",
    "    def gradient(self, X, y):\n",
    "        m = X.shape[0]\n",
    "        y_pred = self.sigmoid(X.dot(self.weights))\n",
    "        return (1 / m) * X.T.dot(y_pred - y)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        n_samples, n_features = X.shape\n",
    "        # # initialize the weight as all 0 vactor\n",
    "        # self.weights = np.zeros(n_features)\n",
    "        # initialize the weight using standard Gaussian random number\n",
    "        self.weights = np.random.randn(n_features)\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        for i in range(self.max_iters):\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            for start in range(0, n_samples, self.batch_size):\n",
    "                end = min(start + self.batch_size, n_samples)\n",
    "                X_batch = X_shuffled[start:end]\n",
    "                y_batch = y_shuffled[start:end]\n",
    "\n",
    "                # Compute gradient and update weights\n",
    "                grad = self.gradient(X_batch, y_batch)\n",
    "                self.weights -= self.learning_rate * grad\n",
    "\n",
    "            # monitor loss\n",
    "            loss = self.compute_loss(X, y)\n",
    "            losses.append(loss)\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Iteration {i}, Loss: {loss:.4f}\")\n",
    "        print(f'\\nfinal loss: {losses[-1]:.4f}')\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.scaler.transform(X)\n",
    "        probabilities = self.sigmoid(X.dot(self.weights))\n",
    "        return (probabilities >= 0.5).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exercise 4: Model implementation:  classification on a breast cancer dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code is credit to https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X0 = breast_cancer_wisconsin_diagnostic.data.features \n",
    "y0 = breast_cancer_wisconsin_diagnostic.data.targets \n",
    "  \n",
    "# metadata \n",
    "#print(breast_cancer_wisconsin_diagnostic.metadata) \n",
    "  \n",
    "# variable information \n",
    "#print(breast_cancer_wisconsin_diagnostic.variables) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the dataframe to numpy array\n",
    "X = X0.to_numpy()\n",
    "y = y0.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Split the dataset into train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "# split the data into train:validation:test = 8:1:1\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size = 0.8, random_state=23)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, train_size = 0.5, random_state=23)\n",
    "\n",
    "# check the result\n",
    "print(f'number of training data: {len(X_train)}')\n",
    "print(f'number of validation data: {len(X_val)}')\n",
    "print(f'number of test data: {len(X_test)}')\n",
    "\n",
    "# check if the data and target length fits\n",
    "assert len(X_train) == len(y_train)\n",
    "assert len(X_val) == len(y_val)\n",
    "assert len(X_test) == len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Report the size of each class in training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of each class in training set:\n",
      "Counter({'B': 282, 'M': 173})\n",
      "\n",
      "size of each class in validation set:\n",
      "Counter({'B': 41, 'M': 16})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# change shape: (n, 1) --> (n,)\n",
    "def count(y):\n",
    "    y_flat = y.flatten()\n",
    "    category_counts = Counter(y_flat)\n",
    "    return category_counts\n",
    "\n",
    "print('size of each class in training set:')\n",
    "print(count(y_train))\n",
    "print('\\nsize of each class in validation set:')\n",
    "print(count(y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the size of each class in both training set and validation is unbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Train our minibatch-SGD model (from exercise 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 1.3285\n",
      "Iteration 100, Loss: 0.0844\n",
      "Iteration 200, Loss: 0.0697\n",
      "Iteration 300, Loss: 0.0633\n",
      "Iteration 400, Loss: 0.0595\n",
      "Iteration 500, Loss: 0.0569\n",
      "Iteration 600, Loss: 0.0551\n",
      "Iteration 700, Loss: 0.0537\n",
      "Iteration 800, Loss: 0.0526\n",
      "Iteration 900, Loss: 0.0517\n",
      "final loss: 0.0509\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# preprocessing: change the label to 0/1\n",
    "# change shape: (n, 1) --> (n,)\n",
    "y_train = y_train.flatten()\n",
    "y_val = y_val.flatten()\n",
    "y_test = y_test.flatten()\n",
    "# encode y: M, B --> 0, 1\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.fit_transform(y_val)\n",
    "y_test_encoded = label_encoder.fit_transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.3402\n",
      "Iteration 100, Loss: 0.0518\n",
      "Iteration 200, Loss: 0.0471\n",
      "Iteration 300, Loss: 0.0446\n",
      "Iteration 400, Loss: 0.0429\n",
      "Iteration 500, Loss: 0.0416\n",
      "Iteration 600, Loss: 0.0406\n",
      "Iteration 700, Loss: 0.0398\n",
      "Iteration 800, Loss: 0.0392\n",
      "Iteration 900, Loss: 0.0386\n",
      "final loss: 0.0381\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model = LogisticRegressionSGD(learning_rate=0.05, batch_size=8, max_iters=1000)\n",
    "losses = model.train(X_train, y_train_encoded)\n",
    "# do prediction on training and validation set\n",
    "train_predictions = model.predict(X_train)\n",
    "val_predictions = model.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to finetune the hpyerparameter and find the best value. We evaluate the model performance using **Macro F-1 score** of the validation set (since the validation data is also strongly imbalanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Macro F1 score on validation set is 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "# present macro f1 score on validation set\n",
    "print(f\"the Macro F1 score on validation set is {f1_score(y_val_encoded, val_predictions, average = 'macro'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the macro F1 score with different hyperparameters:\n",
    "| learning rate | max_iteration | Batch size |   Macro F1    |  Train Loss  |\n",
    "|---------------|---------------|------------|---------------|--------------|\n",
    "|      0.01     |     200       |      16    |     0.9787    |    0.0738    |\n",
    "|      0.01     |     500       |      16    |     0.9787    |    0.0609    |\n",
    "|      0.01     |     1000      |      16    |     0.9787    |    0.0509    |\n",
    "|      0.005    |     1000      |      16    |     0.9787    |    0.0556    |\n",
    "|      0.005    |     1000      |      8     |     1.0000    |    0.0375    |\n",
    "|      0.05     |     1000      |      16    |     1.0000    |    0.0418    |\n",
    "|      0.05     |     1000      |      32    |     0.9787    |    0.0570    |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the validation set is rather small, the macro F1 score are quite close. Thus we take the train loss into consideration and finally decide that **learning rate = 0.005, max iteration = 1000, batch size = 8** to be the final hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. report the performance of the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.96        34\n",
      "           1       0.95      0.91      0.93        23\n",
      "\n",
      "    accuracy                           0.95        57\n",
      "   macro avg       0.95      0.94      0.94        57\n",
      "weighted avg       0.95      0.95      0.95        57\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# run the model on test set to see its performance\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test_encoded, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Summarization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the model performance is very good. The F1 scores on train set and validation set are both close to 1, in test set the score is slightly lower but acceptable. \n",
    "\n",
    "From (c) we can see that the dataset is unbalanced, so using macro F1 score as evaluation is better than simply using accuracy. \n",
    "\n",
    "This is a classification problem, and the 2 classes can be easily splitted, thus the predicted value will be very close to 0 or 1 sometimes. This will cause blowing up and divide by zero in log in the calculation of loss. Thus a small $\\epsilon$ could be added to avoud NAN problems. \n",
    "\n",
    "The loss in training is always decreasing, meaning that the model does not overfit to our data. \n",
    "\n",
    "When choosing hyperparameter, I did several experiment and find that the more iteration we take, the smaller batch size, the smaller learning rate leads to a better performance. However, the training time will also grow. When choosing the hyperparameter, we should balance both side. \n",
    "\n",
    "Because of the rather small dataset and unbalanced spliting(the validation set is too small), The differences in performance when changing hyperparameters are not obvious. Thus we can also see the loss on training set to evaluate model performance. \n",
    "\n",
    "Additionally, we could re-split the train and validation set to bring greater changes in macro F1 score when fintuning our hyperparameter. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
